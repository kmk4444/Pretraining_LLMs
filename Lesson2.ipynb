{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82803,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":69554,"modelId":94690}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lecture 2: Data Preparation\n\nIn this lesson you'll carry out some of the data cleaning steps required to prepare data for pretraining. In the video, Sung mentioned an Upstage tool called **Dataverse** which can help you with data cleaning. You can checkout the features of Dataverse at [this link](https://github.com/UpstageAI/dataverse).","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:16.436089Z","iopub.execute_input":"2024-07-26T15:01:16.436532Z","iopub.status.idle":"2024-07-26T15:01:16.442304Z","shell.execute_reply.started":"2024-07-26T15:01:16.436499Z","shell.execute_reply":"2024-07-26T15:01:16.440785Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## 1. Sourcing datasets for pretraining\n\nIn this section, you'll see two ways to source data for training:\n1. Download an existing dataset from Hugging Face\n2. Create a dataset of python scripts sourced from Github\n\nIn both cases the result will be a Hugging Face `Dataset` object, part of the `Datasets` library. You can read more about the properties of Datasets and how to work with them on the [Hugging Face website](https://huggingface.co/docs/datasets/en/index).\n\n### Download data from Hugging face\n\nThe dataset you download here is a subset of a much larger dataset called **Red Pajama**. The full, 1 trillion token dataset is available on Hugging Face at [this link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T).","metadata":{}},{"cell_type":"code","source":"import datasets\npretraining_dataset = datasets.load_dataset(\n    \"upstage/Pretraining_Dataset\",\n    split=\"train\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:16.452786Z","iopub.execute_input":"2024-07-26T15:01:16.453196Z","iopub.status.idle":"2024-07-26T15:01:27.287709Z","shell.execute_reply.started":"2024-07-26T15:01:16.453166Z","shell.execute_reply":"2024-07-26T15:01:27.286511Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/150M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e108960e59194307812a424dae1d8b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ad1ee8219c41aab21c7fe664a60c91"}},"metadata":{}}]},{"cell_type":"code","source":"print(pretraining_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:27.289955Z","iopub.execute_input":"2024-07-26T15:01:27.290503Z","iopub.status.idle":"2024-07-26T15:01:27.296498Z","shell.execute_reply.started":"2024-07-26T15:01:27.290471Z","shell.execute_reply":"2024-07-26T15:01:27.295171Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text', 'meta'],\n    num_rows: 60000\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Only work with the `text` column:","metadata":{}},{"cell_type":"code","source":"pretraining_dataset = pretraining_dataset.select_columns(\n    [\"text\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:27.298051Z","iopub.execute_input":"2024-07-26T15:01:27.298652Z","iopub.status.idle":"2024-07-26T15:01:27.308928Z","shell.execute_reply.started":"2024-07-26T15:01:27.298611Z","shell.execute_reply":"2024-07-26T15:01:27.307934Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Print a sample:","metadata":{}},{"cell_type":"code","source":"print(pretraining_dataset[0][\"text\"][:500])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:27.311883Z","iopub.execute_input":"2024-07-26T15:01:27.312309Z","iopub.status.idle":"2024-07-26T15:01:27.322000Z","shell.execute_reply.started":"2024-07-26T15:01:27.312264Z","shell.execute_reply":"2024-07-26T15:01:27.320889Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"In 1793 Zaman Shah, a grandson of Ahmad Shah Durrani, won a brief war of succession to become ruler of Afghanistan. The support of Painda Khan, chief of the Baraksai branch of the Durrani tribe, was decisive in his victory. In the next fifty year., the brothers of Zaman shah and the sons of Painda Khan were to dominate the affairs of Afghanistan. The Durrani tribe was very large with several branches and numerous clans. 1 Abmad Shah and his successors belonged to the Sadozai clan, but other clan\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Compare pretraining and fine-tuning datasets\nIn the next cell, you'll download a fine-tuning dataset to contrast with the pretraining dataset you loaded above. You can read more about the Alpaca model and instruction tuning dataset [here](https://crfm.stanford.edu/2023/03/13/alpaca.html). ","metadata":{}},{"cell_type":"code","source":"instruction_dataset = datasets.load_dataset(\n    \"c-s-ale/alpaca-gpt4-data\",\n    split=\"train\"\n)\nprint(instruction_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:27.323581Z","iopub.execute_input":"2024-07-26T15:01:27.324013Z","iopub.status.idle":"2024-07-26T15:01:40.397405Z","shell.execute_reply.started":"2024-07-26T15:01:27.323984Z","shell.execute_reply":"2024-07-26T15:01:40.396199Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"238e55a5f9d74c779fdc34fdc7879087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/43.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2efc438fc4b1444280e296ead194f2c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7027654589844bedbf7c22f4f74ff930"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 52002\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"i=0\nprint(\"Instruction: \" + instruction_dataset[i][\"instruction\"] \n      + \"\\nInput: \" + instruction_dataset[i][\"input\"] \n      + \"\\nOutput: \" + instruction_dataset[i][\"output\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:40.399099Z","iopub.execute_input":"2024-07-26T15:01:40.399527Z","iopub.status.idle":"2024-07-26T15:01:40.408006Z","shell.execute_reply.started":"2024-07-26T15:01:40.399487Z","shell.execute_reply":"2024-07-26T15:01:40.406671Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Instruction: Give three tips for staying healthy.\nInput: \nOutput: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice how in contrast to the pretraining data, which is just raw text, fine-tuning datasets are structured into question-answer pairs or instruction-response sets that can include additional input context if required. \n\nMoving forward, you'll only work with the unstructured pretraining dataset.","metadata":{}},{"cell_type":"markdown","source":"### Scrape python code from Github\nHere, you'll download a selection of python scripts from Github and then prepare them as a Hugging Face `Dataset` object to use in training. \n\nThe same pattern here will work for preparing any text scraped from the web.","metadata":{}},{"cell_type":"code","source":"# Import some required package\nimport os\nimport requests\n\n#Path to directory to store python scripts\ncode_dir = \"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:40.409947Z","iopub.execute_input":"2024-07-26T15:01:40.410287Z","iopub.status.idle":"2024-07-26T15:01:40.514120Z","shell.execute_reply.started":"2024-07-26T15:01:40.410258Z","shell.execute_reply":"2024-07-26T15:01:40.513003Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"urls = [\n    \"https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\",\n    \"https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\",\n    \"https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\",\n    \"https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\",\n    \"https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\",\n    \"https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\",\n    \"https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\",\n    \"https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\",\n    \"https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:40.515537Z","iopub.execute_input":"2024-07-26T15:01:40.516410Z","iopub.status.idle":"2024-07-26T15:01:40.524647Z","shell.execute_reply.started":"2024-07-26T15:01:40.516378Z","shell.execute_reply":"2024-07-26T15:01:40.523548Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Retrieve the python scripts:","metadata":{}},{"cell_type":"code","source":"for url in urls:\n    print(f\"Working on url: {url}\")\n    response = requests.get(url)\n    file_name = os.path.basename(url)\n    file_path = os.path.join(code_dir, file_name)\n    \n    with open(file_path, \"wb\") as file:\n        file.write(response.content)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:40.526108Z","iopub.execute_input":"2024-07-26T15:01:40.526441Z","iopub.status.idle":"2024-07-26T15:01:44.429508Z","shell.execute_reply.started":"2024-07-26T15:01:40.526414Z","shell.execute_reply":"2024-07-26T15:01:44.428339Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Working on url: https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\nWorking on url: https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\nWorking on url: https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\nWorking on url: https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\nWorking on url: https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\nWorking on url: https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\nWorking on url: https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\nWorking on url: https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\nWorking on url: https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\n","output_type":"stream"}]},{"cell_type":"code","source":"files = os.listdir(code_dir)\nfor file in files:\n    print(file)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:44.433881Z","iopub.execute_input":"2024-07-26T15:01:44.434242Z","iopub.status.idle":"2024-07-26T15:01:44.440979Z","shell.execute_reply.started":"2024-07-26T15:01:44.434213Z","shell.execute_reply":"2024-07-26T15:01:44.439783Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"version.py\n.virtual_documents\nnumpy_mlp.py\ntest_subgraph_rewriter.py\ndouble_linear_search_recursion.py\n__init__.py\ndistribute_coordinator_context.py\nmodule_util.py\nvalues.py\nvisualize.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Concatenate scripts into a list:","metadata":{}},{"cell_type":"code","source":"import os\n\ncode_dataset = []\nfor file in os.listdir(code_dir):\n    file_path = os.path.join(code_dir, file)\n    if os.path.isfile(file_path):  # Sadece dosyalar üzerinde işlem yap\n        with open(file_path, 'r') as f:\n            code_dataset.append({'text': f.read()})\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:44.442379Z","iopub.execute_input":"2024-07-26T15:01:44.442740Z","iopub.status.idle":"2024-07-26T15:01:44.451809Z","shell.execute_reply.started":"2024-07-26T15:01:44.442702Z","shell.execute_reply":"2024-07-26T15:01:44.450826Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Convert list to Hugging Face `Dataset` object:","metadata":{}},{"cell_type":"code","source":"code_dataset = datasets.Dataset.from_list(code_dataset)\nprint(code_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:44.453321Z","iopub.execute_input":"2024-07-26T15:01:44.453656Z","iopub.status.idle":"2024-07-26T15:01:44.470299Z","shell.execute_reply.started":"2024-07-26T15:01:44.453627Z","shell.execute_reply":"2024-07-26T15:01:44.468686Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 9\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Combine the python code dataset with the pretraining dataset you downloaded above:","metadata":{}},{"cell_type":"code","source":"dataset = datasets.concatenate_datasets(\n    [pretraining_dataset, code_dataset]\n)\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:44.471828Z","iopub.execute_input":"2024-07-26T15:01:44.472243Z","iopub.status.idle":"2024-07-26T15:01:44.484436Z","shell.execute_reply.started":"2024-07-26T15:01:44.472213Z","shell.execute_reply":"2024-07-26T15:01:44.483263Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 60009\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Data cleaning\n\nIn the cells below, you'll carry out the following cleaning steps:\n1. Filter out samples that are too short\n2. Remove repetitions within a single text example\n3. Remove duplicated documents\n4. Quality filter to remove non-English texts ","metadata":{}},{"cell_type":"code","source":"dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:44.486089Z","iopub.execute_input":"2024-07-26T15:01:44.486466Z","iopub.status.idle":"2024-07-26T15:01:44.495141Z","shell.execute_reply.started":"2024-07-26T15:01:44.486428Z","shell.execute_reply":"2024-07-26T15:01:44.493924Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"60009"},"metadata":{}}]},{"cell_type":"markdown","source":"### Remove examples that are too short","metadata":{}},{"cell_type":"code","source":"import heapq\n\ndef paragraph_length_filter(x):\n    \"\"\"Returns False if a page has too few lines or lines are too short.\"\"\"\n    lines = x['text'].split('\\n')\n    if (\n        len(lines) <3\n        or min(heapq.nlargest(3, [len(line) for line in lines])) < 3\n    ):\n        return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:44.496619Z","iopub.execute_input":"2024-07-26T15:01:44.497049Z","iopub.status.idle":"2024-07-26T15:01:44.504329Z","shell.execute_reply.started":"2024-07-26T15:01:44.497018Z","shell.execute_reply":"2024-07-26T15:01:44.503314Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.filter(\n    paragraph_length_filter,\n    load_from_cache_file=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:44.505997Z","iopub.execute_input":"2024-07-26T15:01:44.506411Z","iopub.status.idle":"2024-07-26T15:01:46.280787Z","shell.execute_reply.started":"2024-07-26T15:01:44.506374Z","shell.execute_reply":"2024-07-26T15:01:46.279698Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/60009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d07e0f2d5b40c4ab0f9196de94be5f"}},"metadata":{}}]},{"cell_type":"code","source":"dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:46.282066Z","iopub.execute_input":"2024-07-26T15:01:46.282385Z","iopub.status.idle":"2024-07-26T15:01:46.289283Z","shell.execute_reply.started":"2024-07-26T15:01:46.282357Z","shell.execute_reply":"2024-07-26T15:01:46.288200Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"52357"},"metadata":{}}]},{"cell_type":"markdown","source":"### Remove repeated text within training examples\n\nHere you'll remove text repetitions within each example. ","metadata":{}},{"cell_type":"code","source":"def find_duplicates(paragraphs):\n    \"\"\"\n    Use this function to find the number of repetitions \n    in the paragraphs.\n    \"\"\"\n    \n    unique_x = set()\n    duplicate_chars = 0\n    duplicate_elements = 0\n    for element in paragraphs:\n        if element in unique_x:\n            duplicate_chars += len(element)\n            duplicate_elements +=1\n        else:\n            unique_x.add(element)\n    return duplicate_elements, duplicate_chars","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:46.290497Z","iopub.execute_input":"2024-07-26T15:01:46.290833Z","iopub.status.idle":"2024-07-26T15:01:46.301328Z","shell.execute_reply.started":"2024-07-26T15:01:46.290789Z","shell.execute_reply":"2024-07-26T15:01:46.300145Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef paragraph_repetition_filter(x):\n    \"\"\"\n    Returns False iff a page has too many repetitions.\n    \"\"\"\n    text = x['text']\n    paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())                # Split by paragraphs (2 or more newlines)\n    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)  # Find number of duplicates in paragraphs\n    if paragraphs_duplicates / len(paragraphs) > 0.3:\n        return False\n    if char_duplicates / len(text) > 0.2:\n        return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:46.302993Z","iopub.execute_input":"2024-07-26T15:01:46.303371Z","iopub.status.idle":"2024-07-26T15:01:46.312612Z","shell.execute_reply.started":"2024-07-26T15:01:46.303336Z","shell.execute_reply":"2024-07-26T15:01:46.311403Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.filter(\n    paragraph_repetition_filter,\n    load_from_cache_file=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:46.313900Z","iopub.execute_input":"2024-07-26T15:01:46.314254Z","iopub.status.idle":"2024-07-26T15:01:52.273016Z","shell.execute_reply.started":"2024-07-26T15:01:46.314221Z","shell.execute_reply":"2024-07-26T15:01:52.271796Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/52357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff280688e7d4573ad6f1f4be2be176b"}},"metadata":{}}]},{"cell_type":"code","source":"dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:52.274969Z","iopub.execute_input":"2024-07-26T15:01:52.275335Z","iopub.status.idle":"2024-07-26T15:01:52.282354Z","shell.execute_reply.started":"2024-07-26T15:01:52.275306Z","shell.execute_reply":"2024-07-26T15:01:52.281160Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"52327"},"metadata":{}}]},{"cell_type":"markdown","source":"### Deduplication\n\nIn this section, you'll remove duplicate examples from the entire dataset (in contrast to the previous step where you were just looking for repeated text in each example.)","metadata":{}},{"cell_type":"code","source":"def deduplication(ds):\n    def dedup_func(x):\n        \"\"\"Use this function to remove duplicate entries\"\"\"\n        if x['text'] in unique_text:\n            return False\n        else:\n            unique_text.add(x['text'])\n            return True\n\n    unique_text = set()\n\n    ds = ds.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n    return ds\n\ndataset = deduplication(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:52.284062Z","iopub.execute_input":"2024-07-26T15:01:52.284499Z","iopub.status.idle":"2024-07-26T15:01:53.924158Z","shell.execute_reply.started":"2024-07-26T15:01:52.284460Z","shell.execute_reply":"2024-07-26T15:01:53.922975Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/52327 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9673d146299440aeae6fc4377f522ec1"}},"metadata":{}}]},{"cell_type":"code","source":"dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:53.925992Z","iopub.execute_input":"2024-07-26T15:01:53.926393Z","iopub.status.idle":"2024-07-26T15:01:53.933731Z","shell.execute_reply.started":"2024-07-26T15:01:53.926359Z","shell.execute_reply":"2024-07-26T15:01:53.932536Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"43598"},"metadata":{}}]},{"cell_type":"markdown","source":"### Quality filter - Language\n\nHere you'll remove any text examples that are in a language other than English. The code here uses a language detection model called fastText. You can read about fastText [here](https://fasttext.cc/).","metadata":{}},{"cell_type":"code","source":"!pip install fasttext","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:01:53.935139Z","iopub.execute_input":"2024-07-26T15:01:53.935514Z","iopub.status.idle":"2024-07-26T15:02:10.179364Z","shell.execute_reply.started":"2024-07-26T15:01:53.935474Z","shell.execute_reply":"2024-07-26T15:02:10.177896Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Requirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (0.9.3)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext) (2.13.1)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext) (69.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fasttext) (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import urllib\nfrom fasttext.FastText import _FastText\n\ndef english_language_filter(ds):\n    # load language detection model\n    model = _FastText('/kaggle/input/l2_language_model.bin/other/default/1/L2_language_model.bin')\n    \n    def is_english(x):\n        # Predict language of the text and probability\n        language, score = model.predict(x['text'].replace(\"\\n\", \"\"))\n\n        language = language[0].split(\"__\")[2]\n        return score > 0.4 and language == \"en\" # change code here if building a model in another language\n\n    ds = ds.filter(is_english, load_from_cache_file=False, num_proc=1)\n    return ds\n\ndataset = english_language_filter(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:10.181334Z","iopub.execute_input":"2024-07-26T15:02:10.181735Z","iopub.status.idle":"2024-07-26T15:02:33.621931Z","shell.execute_reply.started":"2024-07-26T15:02:10.181694Z","shell.execute_reply":"2024-07-26T15:02:33.620779Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"Parameter 'function'=<function english_language_filter.<locals>.is_english at 0x7e2028356290> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/43598 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da2c137274be4d3f98cda9553140f114"}},"metadata":{}}]},{"cell_type":"code","source":"dataset.num_rows","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:33.623483Z","iopub.execute_input":"2024-07-26T15:02:33.623818Z","iopub.status.idle":"2024-07-26T15:02:33.631307Z","shell.execute_reply.started":"2024-07-26T15:02:33.623789Z","shell.execute_reply":"2024-07-26T15:02:33.630197Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"40474"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3. Save the dataset to disk\n\nRead more about the parquet data format [here](https://parquet.apache.org/).","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/working/preprocessed_dataset.parquet\"\ndataset.to_parquet(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:33.632940Z","iopub.execute_input":"2024-07-26T15:02:33.633361Z","iopub.status.idle":"2024-07-26T15:02:35.299490Z","shell.execute_reply.started":"2024-07-26T15:02:33.633324Z","shell.execute_reply":"2024-07-26T15:02:35.298359Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/41 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40f40c4b1c744fcc9e4631c0c7e7ce5a"}},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"197101804"},"metadata":{}}]},{"cell_type":"markdown","source":"# Lesson 3: Data Packaging\n## 1. Tokenizing and creating input_ids\n\nStart by loading the dataset from the previous lesson:","metadata":{}},{"cell_type":"code","source":"import datasets\n\ndataset = datasets.load_dataset(\n    \"parquet\", \n    data_files=\"/kaggle/working/preprocessed_dataset.parquet\", \n    split=\"train\"\n)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:35.306496Z","iopub.execute_input":"2024-07-26T15:02:35.306928Z","iopub.status.idle":"2024-07-26T15:02:36.933223Z","shell.execute_reply.started":"2024-07-26T15:02:35.306886Z","shell.execute_reply":"2024-07-26T15:02:36.931961Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c214bfba458a4070979f1866ed1b2b03"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 40474\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Use the `shard` method of the Hugging Face `Dataset` object to split the dataset into 10 smaller pieces, or *shards* (think shards of broken glass). You can read more about sharding at [this link](https://huggingface.co/docs/datasets/en/process#shard).","metadata":{}},{"cell_type":"code","source":"dataset = dataset.shard(num_shards=10, index=0)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:36.934425Z","iopub.execute_input":"2024-07-26T15:02:36.934839Z","iopub.status.idle":"2024-07-26T15:02:36.947713Z","shell.execute_reply.started":"2024-07-26T15:02:36.934796Z","shell.execute_reply":"2024-07-26T15:02:36.946553Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 4048\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load the tokenizer and try it out:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nmodel_path_or_name = \"upstage/SOLAR-10.7B-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path_or_name,\n    use_fast=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:36.949147Z","iopub.execute_input":"2024-07-26T15:02:36.949546Z","iopub.status.idle":"2024-07-26T15:02:40.358362Z","shell.execute_reply.started":"2024-07-26T15:02:36.949516Z","shell.execute_reply":"2024-07-26T15:02:40.357206Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4c18870aab47a19a7c1e58245869e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c32a218c19814da688f2e2d20a117156"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c3926cd7384aab8104ba52523a48d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827ace4f63f44d6a99a4fc5f689ec81f"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.tokenize(\"I'm a short sentence\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:40.359791Z","iopub.execute_input":"2024-07-26T15:02:40.360270Z","iopub.status.idle":"2024-07-26T15:02:40.368812Z","shell.execute_reply.started":"2024-07-26T15:02:40.360237Z","shell.execute_reply":"2024-07-26T15:02:40.367609Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['▁I', \"'\", 'm', '▁a', '▁short', '▁sentence']"},"metadata":{}}]},{"cell_type":"code","source":"def tokenization(example):\n    #Tokenize\n    tokens = tokenizer.tokenize(example[\"text\"])\n    \n    # Convert tokens to ids\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    \n    # Add <bos>, <eos> tokens to the front and back of tokens_ids \n    # bos: begin of sequence, eos: end of sequence\n    token_ids = [\n        tokenizer.bos_token_id] \\\n        + token_ids \\\n        + [tokenizer.eos_token_id\n    ]\n    example[\"input_ids\"] = token_ids\n    \n    # We will be using this column to count the total number of tokens \n    # in the final dataset\n    example[\"num_tokens\"] = len(token_ids)\n    return example","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:40.370454Z","iopub.execute_input":"2024-07-26T15:02:40.371239Z","iopub.status.idle":"2024-07-26T15:02:40.378108Z","shell.execute_reply.started":"2024-07-26T15:02:40.371198Z","shell.execute_reply":"2024-07-26T15:02:40.377027Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Tokenize Etme:\nİlk adımda, \"Merhaba dünya\" metni token'lere ayrılır. Bu işlem sonucu iki token elde ederiz: [\"Merhaba\", \"dünya\"].\n\nToken'leri ID'lere Çevirme:\nElde edilen token'ler, modelin kelime dağarcığındaki karşılıklarıyla sayısal ID'lere dönüştürülür. Örneğin, \"Merhaba\" token'ı 1234 ve \"dünya\" token'ı 5678 ID'leriyle temsil edilebilir. Böylece token'ler şu hale gelir: [1234, 5678].\n\nÖzel Token'leri Ekleyin:\nBaşlangıç (<bos>) ve bitiş (<eos>) token'lerini eklemek için, ID listesinin başına ve sonuna sırasıyla 101 ve 102 ID'lerini ekleriz. Bu token'ler sırasıyla bos_token_id ve eos_token_id değerlerine karşılık gelir. Sonuç olarak, token ID'leri [101, 1234, 5678, 102] şeklinde olur.\n\nToken Sayısını Hesaplayın:\nSon olarak, toplam token sayısını hesaplarız. Token ID'leri listesi [101, 1234, 5678, 102] olduğundan, bu liste dört token içerir. Bu bilgi example sözlüğüne \"num_tokens\" anahtarı altında eklenir ve değeri 4 olur.\n    \nSonuç olarak, example sözlüğü şu hale gelir:    \n    {\n    \"text\": \"Merhaba dünya\",\n    \"input_ids\": [101, 1234, 5678, 102],\n    \"num_tokens\": 4\n}\n","metadata":{}},{"cell_type":"markdown","source":"Tokenize all the examples in the pretraining dataset:","metadata":{}},{"cell_type":"markdown","source":"Bu kod, Hugging Face datasets kütüphanesi kullanılarak bir veri kümesine tokenizasyon işlemi uygular ve bu işlem sonrası veri kümesini ekrana yazdırır. dataset.map metodu, her bir veri örneği üzerinde tokenization fonksiyonunu uygularken, load_from_cache_file=False parametresi, verilerin her seferinde taze olarak işlenmesini sağlar. print(dataset) ise veri kümesinin son durumunu görüntülemek için kullanılır.\n\nVarsayalım ki dataset'in bazı verileri şu şekildedir:\n\n[{'text': 'Merhaba dünya'}, {'text': 'Nasılsın?'}]\n\ntokenization fonksiyonu uygulandıktan sonra, veri kümesi şu şekilde güncellenmiş olabilir:\n\n[{'text': 'Merhaba dünya', 'input_ids': [101, 1234, 5678, 102], 'num_tokens': 4},\n {'text': 'Nasılsın?', 'input_ids': [101, 6789, 1020, 102], 'num_tokens': 3}]\n","metadata":{}},{"cell_type":"code","source":"dataset = dataset.map(tokenization, load_from_cache_file=False)\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:02:40.379484Z","iopub.execute_input":"2024-07-26T15:02:40.379824Z","iopub.status.idle":"2024-07-26T15:03:31.120105Z","shell.execute_reply.started":"2024-07-26T15:02:40.379795Z","shell.execute_reply":"2024-07-26T15:03:31.119024Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4048 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbbec03bbc50498da12b4a72f74eba34"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['text', 'input_ids', 'num_tokens'],\n    num_rows: 4048\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"sample = dataset[3]\n\nprint(\"text\", sample[\"text\"][:30]) # \nprint(\"\\ninput_ids\", sample[\"input_ids\"][:30])\nprint(\"\\nnum_tokens\", sample[\"num_tokens\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:31.121585Z","iopub.execute_input":"2024-07-26T15:03:31.121945Z","iopub.status.idle":"2024-07-26T15:03:31.129977Z","shell.execute_reply.started":"2024-07-26T15:03:31.121915Z","shell.execute_reply":"2024-07-26T15:03:31.128559Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"text The Colorado Climate Center pr\n\ninput_ids [1, 415, 15837, 1366, 3314, 6064, 5312, 430, 19102, 304, 1178, 356, 281, 3928, 28725, 9735, 28713, 28725, 264, 1052, 14455, 4623, 28725, 9390, 1452, 274, 28725, 17268, 28713, 28725]\n\nnum_tokens 549\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Öncelikle, elimizde şu şekilde bir veri kümesi olduğunu varsayalım:\n\nVeri kümesi, her bir örnekte birkaç token ID'si içeren listelerden oluşuyor. Örneğin, input_ids şöyle görünebilir: [ [1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20] ].\nİlk olarak, tüm token ID'lerini tek bir uzun liste halinde birleştiriyoruz. Bu, tüm örneklerdeki token ID'lerini ardışık olarak sıralamak anlamına gelir. Sonuçta elde edilen liste, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] şeklinde olur.\n\nSonra, bu birleşik listenin uzunluğunu, belirli bir maksimum uzunlukta parçalara bölmek için ayarlıyoruz. Örneğin, maksimum uzunluk olarak 8 belirleyelim. Listenin uzunluğu 20 olduğundan, bu uzunluğu en yakın şekilde 8'in katı olacak şekilde ayarlıyoruz. Bu, listeyi 16 elemana kadar kesmek anlamına gelir, çünkü 20'yi 8'in bir katı olan en yakın değere (16) indirgemek istiyoruz. Kalan son 4 token'ı (17, 18, 19, 20) atıyoruz. Bu işlemden sonra liste [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] olur.\n\nSonraki adımda, bu kesilmiş listeyi, her biri belirlenen maksimum uzunlukta olacak şekilde iki satıra bölüyoruz. Sonuçta, her biri 8 token içeren satırlar elde ediyoruz. Bu, iki satırdan oluşan bir yapı sağlar: [[1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16]].\n\nSon olarak, bu yeniden şekillendirilmiş listeyi Hugging Face datasets kütüphanesinin formatına dönüştürüyoruz. Her satır, bir örneği temsil eder ve veri kümesine uygun formatta yapılandırılmış olur. Bu dönüşüm işlemi tamamlandığında, Hugging Face veri kümesi biçiminde iki örnek içeren yeni bir veri kümesi elde ederiz. Bu veri kümesi, her örneğin token ID'lerini içerir ve modelleme süreçlerinde kullanılmak üzere hazır hale gelir.\n\nBu adımlar, token verilerini işlemeye yönelik genel bir akışı ve veri hazırlık aşamalarını içerir, böylece dil modelleme görevlerinde kullanılacak uygun formatta veri setleri oluşturulur.","metadata":{}},{"cell_type":"markdown","source":"Check the total number of tokens in the dataset:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nnp.sum(dataset[\"num_tokens\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:31.131664Z","iopub.execute_input":"2024-07-26T15:03:31.132158Z","iopub.status.idle":"2024-07-26T15:03:31.172330Z","shell.execute_reply.started":"2024-07-26T15:03:31.132117Z","shell.execute_reply":"2024-07-26T15:03:31.170976Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"5113663"},"metadata":{}}]},{"cell_type":"markdown","source":"Concatenate input_ids for all examples into a single list:","metadata":{}},{"cell_type":"code","source":"input_ids = np.concatenate(dataset[\"input_ids\"])\nprint(len(input_ids))","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:31.173997Z","iopub.execute_input":"2024-07-26T15:03:31.174462Z","iopub.status.idle":"2024-07-26T15:03:35.680776Z","shell.execute_reply.started":"2024-07-26T15:03:31.174415Z","shell.execute_reply":"2024-07-26T15:03:35.679718Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"5113663\n","output_type":"stream"}]},{"cell_type":"code","source":"max_seq_length=32","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:35.682418Z","iopub.execute_input":"2024-07-26T15:03:35.682874Z","iopub.status.idle":"2024-07-26T15:03:35.688439Z","shell.execute_reply.started":"2024-07-26T15:03:35.682816Z","shell.execute_reply":"2024-07-26T15:03:35.687204Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"total_length = len(input_ids) - len(input_ids) % max_seq_length\nprint(total_length)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:35.690017Z","iopub.execute_input":"2024-07-26T15:03:35.690445Z","iopub.status.idle":"2024-07-26T15:03:35.701130Z","shell.execute_reply.started":"2024-07-26T15:03:35.690414Z","shell.execute_reply":"2024-07-26T15:03:35.699887Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"5113632\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Discard extra tokens from end of the list so number of tokens is exactly divisible by `max_seq_length`:","metadata":{}},{"cell_type":"code","source":"input_ids = input_ids[:total_length]\nprint(input_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:35.702479Z","iopub.execute_input":"2024-07-26T15:03:35.702836Z","iopub.status.idle":"2024-07-26T15:03:35.711306Z","shell.execute_reply.started":"2024-07-26T15:03:35.702795Z","shell.execute_reply":"2024-07-26T15:03:35.710028Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"(5113632,)\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\ninput_ids_reshaped.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:35.712894Z","iopub.execute_input":"2024-07-26T15:03:35.713304Z","iopub.status.idle":"2024-07-26T15:03:35.731957Z","shell.execute_reply.started":"2024-07-26T15:03:35.713272Z","shell.execute_reply":"2024-07-26T15:03:35.730780Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(159801, 32)"},"metadata":{}}]},{"cell_type":"code","source":"type(input_ids_reshaped)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:35.733446Z","iopub.execute_input":"2024-07-26T15:03:35.733884Z","iopub.status.idle":"2024-07-26T15:03:35.741558Z","shell.execute_reply.started":"2024-07-26T15:03:35.733823Z","shell.execute_reply":"2024-07-26T15:03:35.740274Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"cell_type":"markdown","source":"Convert to Hugging Face dataset:","metadata":{}},{"cell_type":"code","source":"input_ids_list = input_ids_reshaped.tolist()\npackaged_pretrain_dataset = datasets.Dataset.from_dict(\n    {\"input_ids\": input_ids_list}\n)\nprint(packaged_pretrain_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:35.743064Z","iopub.execute_input":"2024-07-26T15:03:35.743416Z","iopub.status.idle":"2024-07-26T15:03:37.756361Z","shell.execute_reply.started":"2024-07-26T15:03:35.743387Z","shell.execute_reply":"2024-07-26T15:03:37.755090Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids'],\n    num_rows: 159801\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Save the packed dataset to disk","metadata":{}},{"cell_type":"code","source":"packaged_pretrain_dataset.to_parquet(\"/kaggle/working/packaged_pretrain_dataset.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:37.757622Z","iopub.execute_input":"2024-07-26T15:03:37.757995Z","iopub.status.idle":"2024-07-26T15:03:38.066715Z","shell.execute_reply.started":"2024-07-26T15:03:37.757965Z","shell.execute_reply":"2024-07-26T15:03:38.065576Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e0fe08d86884b1f89ec41602615b1dc"}},"metadata":{}},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"21093732"},"metadata":{}}]},{"cell_type":"markdown","source":"# Lesson 4: Preparing your model for training","metadata":{}},{"cell_type":"code","source":"# Ignore insignificant warnings (ex: deprecation warnings)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set a seed value for reproducibility\nimport torch\n\ndef fix_torch_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nfix_torch_seed()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:38.068381Z","iopub.execute_input":"2024-07-26T15:03:38.068740Z","iopub.status.idle":"2024-07-26T15:03:38.077282Z","shell.execute_reply.started":"2024-07-26T15:03:38.068710Z","shell.execute_reply":"2024-07-26T15:03:38.076058Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## 1. Model configuration\n\nYou'll configure models based on Meta's Llama family of models. The transformers library has several tools for working with these models, which you can read about [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama).\n\nStart by creating a `LlamaConfig` object to configure the architecture of the model:","metadata":{}},{"cell_type":"code","source":"from transformers import LlamaConfig\nconfig = LlamaConfig()\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:38.079001Z","iopub.execute_input":"2024-07-26T15:03:38.080121Z","iopub.status.idle":"2024-07-26T15:03:38.090275Z","shell.execute_reply.started":"2024-07-26T15:03:38.080067Z","shell.execute_reply":"2024-07-26T15:03:38.088878Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"LlamaConfig {\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, update parameters to change the model architecture:","metadata":{}},{"cell_type":"code","source":"config.num_hidden_layers = 12 # reduced from 32 to 12\nconfig.hidden_size = 1024 # reduced 1/4 from 4096 to 1024\nconfig.intermediate_size = 4096 # reduced 1/3 from 11008 to 4096\nconfig.num_key_value_heads = 8 # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\nconfig.torch_dtype = \"bfloat16\" # for half-precision training\nconfig.use_cache = False # 'True' is incompatible w/ gradient checkpointing\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:38.091782Z","iopub.execute_input":"2024-07-26T15:03:38.092254Z","iopub.status.idle":"2024-07-26T15:03:38.105224Z","shell.execute_reply.started":"2024-07-26T15:03:38.092221Z","shell.execute_reply":"2024-07-26T15:03:38.103869Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"LlamaConfig {\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 12,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Weight initialization\n\nIn the next sections, you'll explore four different ways to initialize the weights of a model for training:\n1. Random weight initialization\n2. Using an existing model for continued pre-training\n3. Downscaling an existing model\n4. Upscaling an existing model","metadata":{}},{"cell_type":"markdown","source":"### Random weight initialization\n\nRandomly initializing model weights sets all weights to values from a truncated normal distribution with mean 0 and standard deviation of 0.02. Values beyond 2-sigma from the mean are set to 0.","metadata":{}},{"cell_type":"code","source":"from transformers import LlamaForCausalLM\nmodel = LlamaForCausalLM(config)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:38.106599Z","iopub.execute_input":"2024-07-26T15:03:38.107033Z","iopub.status.idle":"2024-07-26T15:03:42.958932Z","shell.execute_reply.started":"2024-07-26T15:03:38.106999Z","shell.execute_reply":"2024-07-26T15:03:42.957718Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 1024)\n    (layers): ModuleList(\n      (0-11): 12 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_nparams(model):\n    \"\"\"Calculate the total number of model parameters\"\"\"\n    nparams = sum(p.numel() for p in model.parameters())\n    print(f\"The total number of parameters is: {nparams}\")\n\nprint_nparams(model)  # 248013824 => 248M","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:03:42.960322Z","iopub.execute_input":"2024-07-26T15:03:42.960638Z","iopub.status.idle":"2024-07-26T15:03:42.968478Z","shell.execute_reply.started":"2024-07-26T15:03:42.960611Z","shell.execute_reply":"2024-07-26T15:03:42.967331Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"The total number of parameters is: 248013824\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Take a look at a sample of the weights in a single layer:","metadata":{}},{"cell_type":"code","source":"layer_name = \"model.layers.0.self_attn.q_proj.weight\"\n\nfor name,param in model.named_parameters():\n    if name == layer_name:\n        print(f\"First 30 weights of layer '{layer_name}':\")\n        print(param.data.view(-1)[:30])\n        break","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:04:23.614102Z","iopub.execute_input":"2024-07-26T15:04:23.614508Z","iopub.status.idle":"2024-07-26T15:04:23.656626Z","shell.execute_reply.started":"2024-07-26T15:04:23.614479Z","shell.execute_reply":"2024-07-26T15:04:23.655228Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"First 30 weights of layer 'model.layers.0.self_attn.q_proj.weight':\ntensor([ 1.5794e-02, -2.2748e-02,  2.0156e-02, -2.6072e-02, -8.3267e-05,\n         8.7432e-03, -9.0255e-04, -4.2442e-02,  1.5337e-02,  1.4482e-02,\n         1.3526e-02,  1.9171e-03, -2.3141e-02, -4.2336e-03,  6.9818e-04,\n         8.9955e-03, -2.0524e-02, -1.3378e-02,  2.3255e-02,  9.5167e-04,\n         2.1053e-02,  1.2794e-02, -7.6783e-03, -3.7832e-03, -8.9180e-03,\n         7.4018e-04, -2.5204e-02, -1.7069e-02,  1.3481e-03,  4.7622e-02])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Try using the model for inference:","metadata":{}},{"cell_type":"code","source":"# Load a tokenizer from Upstage Solar, \n# which is compatible with the Llama-2 tokenizer\nfrom transformers import LlamaTokenizer\nmodel_dir = \"upstage/SOLAR-10.7B-v1.0\"\ntokenizer = LlamaTokenizer.from_pretrained(model_dir)\n\n# Run simple inference with prompt\nfrom transformers import TextStreamer\n\nprompt = \"I am an engineer. I love\"\n\ninputs = tokenizer(prompt,return_tensors=\"pt\").to(model.device)\n\nstreamer = TextStreamer(\n    tokenizer,\n    skip_prompt=True,\n    skip_special_tokens=True\n)\n\noutputs = model.generate(\n    **inputs,\n    streamer=streamer,\n    use_cache=True,\n    max_new_tokens=128,\n    do_sample=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:07:33.441242Z","iopub.execute_input":"2024-07-26T15:07:33.441685Z","iopub.status.idle":"2024-07-26T15:07:55.307122Z","shell.execute_reply.started":"2024-07-26T15:07:33.441652Z","shell.execute_reply":"2024-07-26T15:07:55.306018Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n2024-07-26 15:07:36.721827: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-26 15:07:36.721983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-26 15:07:36.854125: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"possessed possessed possessed possessed possessed possessedcontinuecontinuecontinuecontinuecontinueDownloadџcontinueDownloadcontinueDownloadcontinueertsxE Point remoterts remoterts remoterts갑continuecontinuecontinue wide wide atr wide atr wide wide wide wide wide wide wide wide wide wide wide wideursor otra FC otraopesopesopesopesopesopesopesopesopesopesopes wideopes wideopes wideopes wideopes wideopes wideopes wideopesimpse Library wideopesasterasterasterasterasterasterasterasterasterasterasterasterasterasterasterasterasterasterasteraster primarily primarily primarily primarily primarily primarily primarilyasterasterasterasterasterasterasterasterasterasterasteraster primarilyitä primarilyitä primarilyitä primarilyitä\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Remove the model from memory to avoid crashing the kernel:","metadata":{}},{"cell_type":"code","source":"# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\nimport gc\ndel model\ndel streamer\ndel outputs\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:08:08.424637Z","iopub.execute_input":"2024-07-26T15:08:08.426064Z","iopub.status.idle":"2024-07-26T15:08:08.841025Z","shell.execute_reply.started":"2024-07-26T15:08:08.426018Z","shell.execute_reply":"2024-07-26T15:08:08.839872Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"markdown","source":"### Reuse general pretrained model weights\n\nIf you load an existing model, you can use it as is to continue pretraining on new data.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel_name_or_path = \"upstage/TinySolar-248m-4k\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    device_map=\"cpu\",\n    torch_dtype=torch.bfloat16,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:13:05.538914Z","iopub.execute_input":"2024-07-26T15:13:05.539310Z","iopub.status.idle":"2024-07-26T15:13:24.241881Z","shell.execute_reply.started":"2024-07-26T15:13:05.539279Z","shell.execute_reply":"2024-07-26T15:13:24.240675Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11641fa3b324d9ca5bb692e540abb7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c05247274874c7d8d077e37d4776cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a8f3b31cec475c832f07ece4a1e1ad"}},"metadata":{}}]},{"cell_type":"markdown","source":"Remove the model from memory to avoid crashing the kernel:","metadata":{}},{"cell_type":"code","source":"# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\ndel model\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:13:24.244088Z","iopub.execute_input":"2024-07-26T15:13:24.244540Z","iopub.status.idle":"2024-07-26T15:13:24.642432Z","shell.execute_reply.started":"2024-07-26T15:13:24.244488Z","shell.execute_reply":"2024-07-26T15:13:24.641352Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### Downscaling from a general pretrained model\n\nHere you'll downscale the tinySolar-248m-4k model from a 12 layer model to a 10 layer model.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoConfig\n\nmodel_name_or_path = \"upstage/TinySolar-248m-4k\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    device_map=\"cpu\",\n    torch_dtype=torch.bfloat16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:15:14.716575Z","iopub.execute_input":"2024-07-26T15:15:14.716997Z","iopub.status.idle":"2024-07-26T15:15:19.231251Z","shell.execute_reply.started":"2024-07-26T15:15:14.716967Z","shell.execute_reply":"2024-07-26T15:15:19.230031Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db60659f40b9400f91db9418c71f3ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e972d6d18042414eae5632ccf8488940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef058e25ec8e4f05b9d6ebb260da3db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6a0a336597427abd90189b88df39e5"}},"metadata":{}}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:15:23.881212Z","iopub.execute_input":"2024-07-26T15:15:23.881643Z","iopub.status.idle":"2024-07-26T15:15:23.888975Z","shell.execute_reply.started":"2024-07-26T15:15:23.881609Z","shell.execute_reply":"2024-07-26T15:15:23.887632Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 1024)\n    (layers): ModuleList(\n      (0-11): 12 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print_nparams(model) # 248013824 => 248M","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:15:38.714647Z","iopub.execute_input":"2024-07-26T15:15:38.715640Z","iopub.status.idle":"2024-07-26T15:15:38.721807Z","shell.execute_reply.started":"2024-07-26T15:15:38.715600Z","shell.execute_reply":"2024-07-26T15:15:38.720681Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"The total number of parameters is: 248013824\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Remove the middle two layers (layers 5 and 6) and update the configuration:","metadata":{}},{"cell_type":"code","source":"layers = model.model.layers\nmodel.model.layers = layers[:5] + layers[-5:]\n\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    num_hidden_layers=len(model.model.layers),\n)\nmodel.config=config\n\nprint_nparams(model)# 217601024 => 217M","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clear the memory to avoid crashing the kernel:","metadata":{}},{"cell_type":"code","source":"# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\nimport gc\ndel model\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:18:10.355915Z","iopub.execute_input":"2024-07-26T15:18:10.356638Z","iopub.status.idle":"2024-07-26T15:18:10.763191Z","shell.execute_reply.started":"2024-07-26T15:18:10.356599Z","shell.execute_reply":"2024-07-26T15:18:10.762074Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"66"},"metadata":{}}]},{"cell_type":"markdown","source":"### Depth Upscaling from a general pretrained model\n\nHere you are going to upscale the tinySolar-248m-4k model from 12 layers to 16 layers. Here are the steps you'll take:\n1. Configure a 16 layer model and initialize it with random weights\n2. Load the 12 layer tinySolar-248m-4k model into memory\n3. Copy the bottom 8 and top 8 layers from the 12 layer model and use them to overwrite the random weights of the 16 layer model\n4. Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 16 layer model","metadata":{}},{"cell_type":"code","source":"config = LlamaConfig(\n    num_hidden_layers=16,  # We want our model to have 16 final layers\n    hidden_size=1024,\n    intermediate_size=4096,\n    num_attention_heads=32,\n    num_key_value_heads=8,\n    torch_dtype=\"bfloat16\",\n    use_cache=False \n)\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:20:55.423765Z","iopub.execute_input":"2024-07-26T15:20:55.424218Z","iopub.status.idle":"2024-07-26T15:20:55.432981Z","shell.execute_reply.started":"2024-07-26T15:20:55.424183Z","shell.execute_reply":"2024-07-26T15:20:55.431738Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"LlamaConfig {\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = LlamaForCausalLM(config)\nmodel = model.to(dtype=torch.bfloat16)  # convert to bfloat16\nprint_nparams(model)  # 308839424 => 308M","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:21:08.314767Z","iopub.execute_input":"2024-07-26T15:21:08.315202Z","iopub.status.idle":"2024-07-26T15:21:14.012270Z","shell.execute_reply.started":"2024-07-26T15:21:08.315167Z","shell.execute_reply":"2024-07-26T15:21:14.011060Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"The total number of parameters is: 308839424\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name_or_path = \"upstage/TinySolar-248m-4k\"\npretrained_model = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    device_map=\"cpu\",\n    torch_dtype=torch.bfloat16,    \n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\nprint_nparams(pretrained_model) #  248013824 => 248M","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:22:02.253800Z","iopub.execute_input":"2024-07-26T15:22:02.254248Z","iopub.status.idle":"2024-07-26T15:22:03.534024Z","shell.execute_reply.started":"2024-07-26T15:22:02.254214Z","shell.execute_reply":"2024-07-26T15:22:03.532835Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"The total number of parameters is: 248013824\n","output_type":"stream"}]},{"cell_type":"code","source":"from copy import deepcopy\n\nmodel.model.layers = deepcopy(pretrained_model.model.layers[:-4]) \\\n    + deepcopy(pretrained_model.model.layers[4:])\n\nmodel.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n\nmodel.lm_head = deepcopy(pretrained_model.lm_head)\n\nprint(model.config)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:22:28.749727Z","iopub.execute_input":"2024-07-26T15:22:28.750585Z","iopub.status.idle":"2024-07-26T15:22:28.987702Z","shell.execute_reply.started":"2024-07-26T15:22:28.750541Z","shell.execute_reply":"2024-07-26T15:22:28.986677Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"LlamaConfig {\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Check the number of parameters is still 308 million:","metadata":{}},{"cell_type":"code","source":"print_nparams(model)  # 308839424 => 308M","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:22:38.216157Z","iopub.execute_input":"2024-07-26T15:22:38.216723Z","iopub.status.idle":"2024-07-26T15:22:38.225017Z","shell.execute_reply.started":"2024-07-26T15:22:38.216678Z","shell.execute_reply":"2024-07-26T15:22:38.223410Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"The total number of parameters is: 308839424\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Try using the model for inference:","metadata":{}},{"cell_type":"code","source":"# Run simple inference to show no trained model\nprompt = \"I am an engineer. I love\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\nstreamer = TextStreamer(\n    tokenizer, \n    skip_prompt=True, \n    skip_special_tokens=True\n)\n\noutputs = model.generate(\n    **inputs, \n    streamer=streamer, \n    use_cache=True, \n    max_new_tokens=128, \n    do_sample=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:22:58.295632Z","iopub.execute_input":"2024-07-26T15:22:58.296912Z","iopub.status.idle":"2024-07-26T15:23:39.078170Z","shell.execute_reply.started":"2024-07-26T15:22:58.296845Z","shell.execute_reply":"2024-07-26T15:23:39.077110Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"to work with people who are not afraid to look at the world and are not afraid to look at the world with a little bit of a twist.\nI am a very humble person and I am very fortunate to have a great team of people who work hard to make a difference.\nI am very fortunate to have a great team of people who work hard to make a difference.\nI am very fortunate to have a great team of people who work hard to make a difference.\nI am very fortunate to have a great team of people who work hard to make a difference.\nI am very fortunate to have a great team\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save the model to disk\n\nNote the new model name here which reflects the 308 million parameters of the new, upscaled model. ","metadata":{}},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/TinySolar-308m-4k-init')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T15:24:29.754875Z","iopub.execute_input":"2024-07-26T15:24:29.755294Z","iopub.status.idle":"2024-07-26T15:24:30.605556Z","shell.execute_reply.started":"2024-07-26T15:24:29.755261Z","shell.execute_reply":"2024-07-26T15:24:30.604404Z"},"trusted":true},"execution_count":75,"outputs":[]}]}